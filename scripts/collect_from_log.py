# scripts/collect_from_log.py
#!/usr/bin/env python3
"""
Extracts benchmark results (TTFT, Throughput, etc.) from a log file
generated by bench_pd.py / bench_agg.sh / bench_proxy.sh, and saves them as a CSV.
"""

import argparse
import csv
import re
import os
from bench_utils import metadata_from_filename  # <-- shared

# --- Patterns used to parse metrics in logs ---

TTFT_BLOCK = re.compile(
    r"==\s*TTFT.*?N\s*=\s*(?P<N>\d+)\s*==.*?"
    r"p50=(?P<p50>[0-9.]+)s\s+p95=(?P<p95>[0-9.]+)s\s+min=(?P<min>[0-9.]+)s\s+max=(?P<max>[0-9.]+)s",
    re.DOTALL
)

THR_BLOCK = re.compile(
    r"==\s*Throughput.*?N\s*=\s*(?P<N>\d+)\s*,\s*errors\s*=\s*(?P<errors>\d+)\s*==.*?"
    r"per-request.*?p50=(?P<p50>[0-9.]+)\s+p95=(?P<p95>[0-9.]+)\s+mean=(?P<mean>[0-9.]+).*?"
    r"total generated tokens\s*=\s*(?P<tokens>\d+).*?"
    r"wall time\s*\(whole run\)\s*=\s*(?P<wall>[0-9.]+)s.*?"
    r"aggregate throughput\s*=\s*(?P<agg>[0-9.]+)",
    re.DOTALL
)

THR_BLOCK_NO_ERRORS = re.compile(
    r"==\s*Throughput.*?==.*?"
    r"per-request.*?p50=(?P<p50>[0-9.]+)\s+p95=(?P<p95>[0-9.]+)\s+mean=(?P<mean>[0-9.]+).*?"
    r"total generated tokens\s*=\s*(?P<tokens>\d+).*?"
    r"wall time\s*\(whole run\)\s*=\s*(?P<wall>[0-9.]+)s.*?"
    r"aggregate throughput\s*=\s*(?P<agg>[0-9.]+)",
    re.DOTALL
)

def parse_log(log_path: str):
    """Parse a single log file and extract benchmark statistics (both agg and disagg)."""
    data = {
        # From filename (if available)
        **{k: None for k in [
            "mode","model_tag","concurrency","prompt_tokens","max_tokens"
        ]},
        # TTFT
        "requests_ttft": None, "p50_ttft": None, "p95_ttft": None, "min_ttft": None, "max_ttft": None,
        # Throughput
        "requests_thr": None, "errors": None, "p50_tps": None, "p95_tps": None, "mean_tps": None,
        "total_tokens": None, "wall_time_sec": None, "aggregate_throughput": None,
    }

    # filename-derived metadata
    meta = metadata_from_filename(log_path)
    data.update(meta)

    with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
        log = f.read()

    # TTFT
    ttft = TTFT_BLOCK.search(log)
    if ttft:
        data["requests_ttft"] = int(ttft.group("N"))
        data["p50_ttft"] = float(ttft.group("p50"))
        data["p95_ttft"] = float(ttft.group("p95"))
        data["min_ttft"] = float(ttft.group("min"))
        data["max_ttft"] = float(ttft.group("max"))

    # Throughput
    thr = THR_BLOCK.search(log) or THR_BLOCK_NO_ERRORS.search(log)
    if thr:
        if "N" in thr.groupdict() and thr.group("N") is not None:
            data["requests_thr"] = int(thr.group("N"))
        if "errors" in thr.groupdict() and thr.group("errors") is not None:
            data["errors"] = int(thr.group("errors"))
        data["p50_tps"] = float(thr.group("p50"))
        data["p95_tps"] = float(thr.group("p95"))
        data["mean_tps"] = float(thr.group("mean"))
        data["total_tokens"] = int(thr.group("tokens"))
        data["wall_time_sec"] = float(thr.group("wall"))
        data["aggregate_throughput"] = float(thr.group("agg"))

    return data


def save_to_csv(data: dict, output_path: str):
    """Write the parsed data dictionary to a single-row CSV."""
    cols = [
        # meta
        "mode","model_tag","concurrency","prompt_tokens","max_tokens",
        # ttft
        "requests_ttft","p50_ttft","p95_ttft","min_ttft","max_ttft",
        # throughput
        "requests_thr","errors","p50_tps","p95_tps","mean_tps",
        "total_tokens","wall_time_sec","aggregate_throughput",
    ]
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=cols)
        writer.writeheader()
        writer.writerow({k: data.get(k) for k in cols})


def main():
    parser = argparse.ArgumentParser(description="Extract benchmark results from bench logs (agg & disagg).")
    parser.add_argument("--input", required=True, help="Path to the log file")
    parser.add_argument("--output", required=True, help="Path to the output CSV file")
    args = parser.parse_args()

    parsed = parse_log(args.input)
    save_to_csv(parsed, args.output)
    print(f"[OK] Extracted metrics saved to {args.output}")


if __name__ == "__main__":
    main()