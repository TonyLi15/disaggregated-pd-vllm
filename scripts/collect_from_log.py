#!/usr/bin/env python3
"""
collect_from_log.py
-------------------
Extracts benchmark results (TTFT, Throughput, etc.) from a log file
generated by bench_pd.py / bench_agg.sh / bench_proxy.sh, and saves them as a CSV.

It supports both disaggregated and aggregated runs. It also attempts to
parse the run metadata (mode, model tag, concurrency, prompt tokens, max tokens)
from the filename pattern used by run_bench_vars.sh:

  results/bench_runs/run_{mode}_model_{MODEL_TAG}_conc{C}_pt{P}_mt{M}.log

Example usage:
  python3 scripts/collect_from_log.py \
      --input results/bench_runs/run_disagg_model_Qwen_Qwen2.5-7B-Instruct_conc16_pt512_mt1024.log \
      --output results/bench_runs/run_disagg_model_Qwen_Qwen2.5-7B-Instruct_conc16_pt512_mt1024.csv
"""

import argparse
import csv
import re
import os

# --- Patterns used to parse metrics in logs ---

# TTFT block, e.g.:
# == TTFT (stream=true) N=100 ==
#   p50=0.656s  p95=0.796s  min=0.077s  max=0.800s
TTFT_BLOCK = re.compile(
    r"==\s*TTFT.*?N\s*=\s*(?P<N>\d+)\s*==.*?"
    r"p50=(?P<p50>[0-9.]+)s\s+p95=(?P<p95>[0-9.]+)s\s+min=(?P<min>[0-9.]+)s\s+max=(?P<max>[0-9.]+)s",
    re.DOTALL
)

# Throughput block, e.g.:
# == Throughput (stream=false) N=100, errors=0 ==
#   per-request tokens/sec: p50=67.6  p95=73.3  mean=65.2
#   total generated tokens = 8219
#   wall time (whole run)   = 8.53s
#   aggregate throughput    = 964.1 tokens/sec
THR_BLOCK = re.compile(
    r"==\s*Throughput.*?N\s*=\s*(?P<N>\d+)\s*,\s*errors\s*=\s*(?P<errors>\d+)\s*==.*?"
    r"per-request.*?p50=(?P<p50>[0-9.]+)\s+p95=(?P<p95>[0-9.]+)\s+mean=(?P<mean>[0-9.]+).*?"
    r"total generated tokens\s*=\s*(?P<tokens>\d+).*?"
    r"wall time\s*\(whole run\)\s*=\s*(?P<wall>[0-9.]+)s.*?"
    r"aggregate throughput\s*=\s*(?P<agg>[0-9.]+)",
    re.DOTALL
)

# Fallback throughput block without the "errors=..." piece (older logs)
THR_BLOCK_NO_ERRORS = re.compile(
    r"==\s*Throughput.*?==.*?"
    r"per-request.*?p50=(?P<p50>[0-9.]+)\s+p95=(?P<p95>[0-9.]+)\s+mean=(?P<mean>[0-9.]+).*?"
    r"total generated tokens\s*=\s*(?P<tokens>\d+).*?"
    r"wall time\s*\(whole run\)\s*=\s*(?P<wall>[0-9.]+)s.*?"
    r"aggregate throughput\s*=\s*(?P<agg>[0-9.]+)",
    re.DOTALL
)

# Filename metadata pattern created by run_bench_vars.sh:
# run_{mode}_model_{MODEL_TAG}_conc{C}_pt{P}_mt{M}.log
FNAME_META = re.compile(
    r"run_(?P<mode>agg|disagg)_model_(?P<modeltag>.+?)_conc(?P<conc>\d+)_pt(?P<pt>\d+)_mt(?P<mt>\d+)\.log$"
)

def parse_log(log_path: str):
    """Parse a single log file and extract benchmark statistics (both agg and disagg)."""
    data = {
        # From filename (if available)
        "mode": None,                 # "agg" or "disagg"
        "model_tag": None,
        "concurrency": None,
        "prompt_tokens": None,
        "max_tokens": None,

        # From TTFT section
        "requests_ttft": None,
        "p50_ttft": None,
        "p95_ttft": None,
        "min_ttft": None,
        "max_ttft": None,

        # From Throughput section
        "requests_thr": None,
        "errors": None,
        "p50_tps": None,
        "p95_tps": None,
        "mean_tps": None,
        "total_tokens": None,
        "wall_time_sec": None,
        "aggregate_throughput": None,
    }

    # Try to parse metadata from filename
    fname = os.path.basename(log_path)
    m = FNAME_META.search(fname)
    if m:
        data["mode"] = m.group("mode")
        data["model_tag"] = m.group("modeltag")
        data["concurrency"] = int(m.group("conc"))
        data["prompt_tokens"] = int(m.group("pt"))
        data["max_tokens"] = int(m.group("mt"))

    with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
        log = f.read()

    # TTFT
    ttft = TTFT_BLOCK.search(log)
    if ttft:
        data["requests_ttft"] = int(ttft.group("N"))
        data["p50_ttft"] = float(ttft.group("p50"))
        data["p95_ttft"] = float(ttft.group("p95"))
        data["min_ttft"] = float(ttft.group("min"))
        data["max_ttft"] = float(ttft.group("max"))

    # Throughput (prefer block that includes errors)
    thr = THR_BLOCK.search(log) or THR_BLOCK_NO_ERRORS.search(log)
    if thr:
        # If THR_BLOCK matched, it has N and errors; if NO_ERRORS matched, groups may be missing.
        if "N" in thr.groupdict() and thr.group("N") is not None:
            data["requests_thr"] = int(thr.group("N"))
        if "errors" in thr.groupdict() and thr.group("errors") is not None:
            data["errors"] = int(thr.group("errors"))

        data["p50_tps"] = float(thr.group("p50"))
        data["p95_tps"] = float(thr.group("p95"))
        data["mean_tps"] = float(thr.group("mean"))
        data["total_tokens"] = int(thr.group("tokens"))
        data["wall_time_sec"] = float(thr.group("wall"))
        data["aggregate_throughput"] = float(thr.group("agg"))

    return data


def save_to_csv(data: dict, output_path: str):
    """Write the parsed data dictionary to a single-row CSV."""
    # Keep columns stable and human-useful
    cols = [
        # meta
        "mode", "model_tag", "concurrency", "prompt_tokens", "max_tokens",
        # ttft
        "requests_ttft", "p50_ttft", "p95_ttft", "min_ttft", "max_ttft",
        # throughput
        "requests_thr", "errors", "p50_tps", "p95_tps", "mean_tps",
        "total_tokens", "wall_time_sec", "aggregate_throughput",
    ]
    with open(output_path, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=cols)
        writer.writeheader()
        writer.writerow({k: data.get(k) for k in cols})


def main():
    parser = argparse.ArgumentParser(description="Extract benchmark results from bench logs (agg & disagg).")
    parser.add_argument("--input", required=True, help="Path to the log file")
    parser.add_argument("--output", required=True, help="Path to the output CSV file")
    args = parser.parse_args()

    parsed = parse_log(args.input)
    save_to_csv(parsed, args.output)
    print(f"[OK] Extracted metrics saved to {args.output}")


if __name__ == "__main__":
    main()